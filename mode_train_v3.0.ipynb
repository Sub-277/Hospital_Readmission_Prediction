{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "566e5c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"diabetic_data_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7273dd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99329, 21)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61c5f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'num_medications', 'diag_1', 'time_in_hospital', 'diag_2', 'diag_3',\n",
    "    'age', 'discharge_disposition_id', 'number_diagnoses',\n",
    "    'number_preceding_year_visits', 'num_procedures', 'num_lab_procedures',\n",
    "    'number_diabetes_meds', 'race', 'insulin', 'admission_type_id'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e47812bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "class FeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def _map_diag(self, val):\n",
    "        try:\n",
    "            val = float(val)\n",
    "        except:\n",
    "            return 0\n",
    "        if 1 <= val < 140: return 1\n",
    "        elif 140 <= val < 240: return 2\n",
    "        elif 240 <= val < 280: return 3\n",
    "        elif 280 <= val < 290: return 4\n",
    "        elif 290 <= val < 320: return 5\n",
    "        elif 320 <= val < 390: return 6\n",
    "        elif 390 <= val < 460: return 7\n",
    "        elif 460 <= val < 520: return 8\n",
    "        elif 520 <= val < 580: return 9\n",
    "        elif 580 <= val < 630: return 10\n",
    "        elif 630 <= val < 680: return 11\n",
    "        elif 680 <= val < 710: return 12\n",
    "        elif 710 <= val < 740: return 13\n",
    "        elif 740 <= val < 760: return 14\n",
    "        elif 760 <= val < 780: return 15\n",
    "        elif 780 <= val < 800: return 16\n",
    "        elif 800 <= val < 1000: return 17\n",
    "        return 0\n",
    "\n",
    "    def _map_age(self, val):\n",
    "        # Accept both [60-70) or raw numeric like 63\n",
    "        if isinstance(val, str) and \"[\" in val:\n",
    "            val = val.strip(\"[]\").split(\"-\")\n",
    "            return (int(val[0]) + int(val[1])) // 2\n",
    "        else:\n",
    "            val = float(val)\n",
    "            val = np.clip(val, 0, 100)\n",
    "            return int(((val // 10) * 10) + 5)\n",
    "\n",
    "    def _map_insulin(self, val):\n",
    "        return {'No': -2, 'Down': -1, 'Steady': 0, 'Up': 1}.get(val, -2)\n",
    "\n",
    "    def _map_race(self, val):\n",
    "        return {\n",
    "            'Caucasian': 0, 'AfricanAmerican': 1,\n",
    "            'Asian': 2, 'Hispanic': 3, 'Other': 4\n",
    "        }.get(val, 4)\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['diag_1'] = X['diag_1'].apply(self._map_diag)\n",
    "        X['diag_2'] = X['diag_2'].apply(self._map_diag)\n",
    "        X['diag_3'] = X['diag_3'].apply(self._map_diag)\n",
    "        X['age'] = X['age'].apply(self._map_age)\n",
    "        X['insulin'] = X['insulin'].apply(self._map_insulin)\n",
    "        X['race'] = X['race'].apply(self._map_race)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f2e161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SUBARNA PAUL\\Desktop\\Diabetic_Project\\ML PRACTICAL\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\SUBARNA PAUL\\Desktop\\Diabetic_Project\\ML PRACTICAL\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\SUBARNA PAUL\\Desktop\\Diabetic_Project\\ML PRACTICAL\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [18:55:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9050, number of negative: 70413\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014597 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 240\n",
      "[LightGBM] [Info] Number of data points in the train set: 79463, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.113889 -> initscore=-2.051613\n",
      "[LightGBM] [Info] Start training from score -2.051613\n",
      "  Model  Precision  Recall      F1  Accuracy  ROC_AUC  Training_Time (s)  \\\n",
      "0   LGB     0.5714  0.0053  0.0105    0.8862   0.6534             1.0280   \n",
      "1   ADA     0.0000  0.0000  0.0000    0.8861   0.6419             3.3094   \n",
      "2   XGB     0.4432  0.0172  0.0332    0.8856   0.6398             0.5181   \n",
      "3    RF     0.5882  0.0044  0.0088    0.8862   0.6282             4.8135   \n",
      "4    LR     0.0000  0.0000  0.0000    0.8861   0.6277             1.3484   \n",
      "5    DT     0.1496  0.1887  0.1669    0.7854   0.5254             1.2338   \n",
      "\n",
      "   Prediction_Time (s)  \n",
      "0               0.0730  \n",
      "1               0.1190  \n",
      "2               0.0333  \n",
      "3               0.1686  \n",
      "4               0.0309  \n",
      "5               0.0217  \n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score,\n",
    "    accuracy_score, roc_auc_score\n",
    ")\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "X = df[features].copy()\n",
    "y = df['readmitted'].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ----------------------------\n",
    "# Define and Evaluate Models\n",
    "# ----------------------------\n",
    "models = []\n",
    "\n",
    "models.append(('LR', Pipeline([\n",
    "    (\"Transformer\", PowerTransformer()),\n",
    "    (\"Scaler\", StandardScaler()),\n",
    "    (\"LogReg\", LogisticRegression(random_state=0, solver='liblinear'))\n",
    "])))\n",
    "\n",
    "models.append(('DT', DecisionTreeClassifier(random_state=0)))\n",
    "models.append(('RF', RandomForestClassifier(random_state=0, n_jobs=-1)))\n",
    "models.append(('ADA', AdaBoostClassifier(random_state=0)))\n",
    "models.append(('XGB', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=0, n_jobs=-1)))\n",
    "models.append(('LGB', LGBMClassifier(random_state=0, n_jobs=-1))) \n",
    "\n",
    "precision, recall, f1, accuracy, roc_auc = [], [], [], [], []\n",
    "Training_Time, Prediction_Time, names = [], [], []\n",
    "\n",
    "for name, model in models:\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    Training_Time.append(time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    Prediction_Time.append(time.time() - start)\n",
    "    \n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    precision.append(precision_score(y_test, y_pred))\n",
    "    recall.append(recall_score(y_test, y_pred))\n",
    "    f1.append(f1_score(y_test, y_pred))\n",
    "    accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    roc_auc.append(roc_auc_score(y_test, y_proba))\n",
    "    names.append(name)\n",
    "\n",
    "# ----------------------------\n",
    "# Results Summary\n",
    "# ----------------------------\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': names,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1': f1,\n",
    "    'Accuracy': accuracy,\n",
    "    'ROC_AUC': roc_auc,\n",
    "    'Training_Time (s)': Training_Time,\n",
    "    'Prediction_Time (s)': Prediction_Time\n",
    "}).sort_values(by='ROC_AUC', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a303406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SUBARNA PAUL\\Desktop\\Diabetic_Project\\ML PRACTICAL\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\SUBARNA PAUL\\Desktop\\Diabetic_Project\\ML PRACTICAL\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [18:55:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9050, number of negative: 70413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 242\n",
      "[LightGBM] [Info] Number of data points in the train set: 79463, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "  Model  Precision   Recall       F1  Accuracy  ROC_AUC  Training_Time (s)  \\\n",
      "0   LGB    16.9936  58.9483  26.3819   62.5239  65.2943             0.3841   \n",
      "1   XGB    17.1306  49.4918  25.4517   66.9737  63.6206             0.3380   \n",
      "2   ADA     0.0000   0.0000   0.0000   88.6087  63.5678             3.2069   \n",
      "3    RF    71.4286   0.4419   0.8783   88.6389  62.6235             4.1059   \n",
      "4    LR    15.4121  57.1807  24.2800   59.3728  61.7511             2.0192   \n",
      "5    DT    14.5455  14.4940  14.5197   80.5598  51.7743             0.9825   \n",
      "\n",
      "   Prediction_Time (s)  \n",
      "0               0.0267  \n",
      "1               0.0144  \n",
      "2               0.0680  \n",
      "3               0.1932  \n",
      "4               0.0402  \n",
      "5               0.0077  \n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score,\n",
    "    accuracy_score, roc_auc_score\n",
    ")\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "X = df[features].copy()\n",
    "y = df['readmitted'].copy()\n",
    "\n",
    "# Class weight calculations\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "neg, pos = np.bincount(y)\n",
    "scale_pos_weight = neg / pos\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=0\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Define Models with Weighting\n",
    "# ----------------------------\n",
    "models_weighted = [\n",
    "    ('LR', Pipeline([\n",
    "        (\"Transformer\", PowerTransformer()),\n",
    "        (\"Scaler\", StandardScaler()),\n",
    "        (\"LogReg\", LogisticRegression(random_state=0, solver='liblinear', class_weight='balanced'))\n",
    "    ])),\n",
    "    ('DT', DecisionTreeClassifier(random_state=0, class_weight='balanced')),\n",
    "    ('RF', RandomForestClassifier(random_state=0, n_jobs=-1, class_weight='balanced')),\n",
    "    ('ADA', AdaBoostClassifier(random_state=0)),  # No class_weight param\n",
    "    ('XGB', XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                          random_state=0, n_jobs=-1, scale_pos_weight=scale_pos_weight)),\n",
    "    ('LGB', LGBMClassifier(random_state=0, n_jobs=-1, class_weight='balanced'))\n",
    "]\n",
    "\n",
    "# ----------------------------\n",
    "# Model Evaluation\n",
    "# ----------------------------\n",
    "results = []\n",
    "for name, model in models_weighted:\n",
    "    t0 = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - t0\n",
    "\n",
    "    t0 = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    prediction_time = time.time() - t0\n",
    "\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Precision': precision_score(y_test, y_pred)*100,\n",
    "        'Recall': recall_score(y_test, y_pred)*100,\n",
    "        'F1': f1_score(y_test, y_pred)*100,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred)*100,\n",
    "        'ROC_AUC': roc_auc_score(y_test, y_proba)*100,\n",
    "        'Training_Time (s)': training_time,\n",
    "        'Prediction_Time (s)': prediction_time\n",
    "    })\n",
    "\n",
    "results_weighted_df = pd.DataFrame(results).sort_values(by='ROC_AUC', ascending=False).reset_index(drop=True)\n",
    "print(results_weighted_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bed408e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9050, number of negative: 70413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002435 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 194\n",
      "[LightGBM] [Info] Number of data points in the train set: 79463, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.64      0.76     17603\n",
      "           1       0.17      0.56      0.26      2263\n",
      "\n",
      "    accuracy                           0.63     19866\n",
      "   macro avg       0.54      0.60      0.51     19866\n",
      "weighted avg       0.83      0.63      0.70     19866\n",
      "\n",
      "Confusion Matrix\n",
      "[[11322  6281]\n",
      " [  994  1269]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SUBARNA PAUL\\Desktop\\Diabetic_Project\\ML PRACTICAL\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "X = df[features].copy()\n",
    "y = df['readmitted'].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"transformer\", FeatureTransformer()),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"classifier\", LGBMClassifier(\n",
    "        class_weight='balanced',\n",
    "        n_estimators=200,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e39780c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hospital_readmission_pipeline_v3.0.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(pipeline, \"hospital_readmission_pipeline_v3.0.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "907879f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SUBARNA PAUL\\Desktop\\Diabetic_Project\\ML PRACTICAL\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\SUBARNA PAUL\\Desktop\\Diabetic_Project\\ML PRACTICAL\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "1\n",
      "Prediction: ðŸ”´ Readmitted\n",
      "Probability of Readmission: 75.89%\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "pipeline = joblib.load(\"hospital_readmission_pipeline_v3.0.pkl\")\n",
    "\n",
    "# New record\n",
    "input_data = {\n",
    "    'num_medications': 15,\n",
    "    'diag_1': '296',\n",
    "    'time_in_hospital': 2,\n",
    "    'diag_2': '427',\n",
    "    'diag_3': '250.02',\n",
    "    'age': 35,\n",
    "    'discharge_disposition_id': 1,\n",
    "    'number_diagnoses': 3,\n",
    "    'number_preceding_year_visits': 6,\n",
    "    'num_procedures': 0,\n",
    "    'num_lab_procedures': 18,\n",
    "    'number_diabetes_meds': 1,\n",
    "    'race': 'Caucasian',\n",
    "    'insulin': 'Steady',\n",
    "    'admission_type_id': 1\n",
    "}\n",
    "\n",
    "df_input = pd.DataFrame([input_data])\n",
    "\n",
    "prediction = pipeline.predict(df_input)[0]\n",
    "proba = pipeline.predict_proba(df_input)[0][1]\n",
    "\n",
    "print(\"=================================\")\n",
    "print(prediction)\n",
    "print(f\"Prediction: {'ðŸ”´ Readmitted' if prediction==1 else 'ðŸŸ¢ Not Readmitted'}\")\n",
    "print(f\"Probability of Readmission: {proba:.2%}\")\n",
    "print(\"=================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425d24be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
